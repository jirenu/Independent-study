{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sense2vec\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "from gensim.test.utils import datapath\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenwords = KeyedVectors.load_word2vec_format(datapath('/home/jrenugopal/guys-NLP/rcv1.oscca.100k.200.c2'), binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2v = sense2vec.load()\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output):\n",
    "        super(Net, self).__init__()                    # Inherited from the parent class nn.Module\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)  # 1st Full-Connected Layer: 784 (input data) -> 500 (hidden node)\n",
    "        self.relu = nn.ReLU()                          # Non-Linear ReLU Layer: max(0,x)\n",
    "        self.fc2 = nn.Linear(hidden_size, output) # 2nd Full-Connected Layer: 500 (hidden node) -> 10 (output class)\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size) \n",
    "    \n",
    "    def forward(self, x):                              # Forward pass: stacking each layer together\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_full = np.array([np.array(j) for j in [0,1,2,3,4,5,6,7,8,9,10, 11,12,13,14]])\n",
    "y_full = np.array([np.array(4*j+5) for j in [0,1,2,3,4,5,6,7,8,9,10, 11,12,13,14]])\n",
    "_,full_dataset = convToDataloader(x_full, y_full)\n",
    "train_data, test_data = torch.utils.data.random_split(full_dataset, [10, 5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(dataset):\n",
    "    return utils.DataLoader(dataset)\n",
    "import torch.utils.data as utils\n",
    "def convToDataloader(x, y):\n",
    "    tensor_x = torch.from_numpy(x) # transform to torch tensors\n",
    "    tensor_y = torch.from_numpy(y)\n",
    "    my_dataset = utils.TensorDataset(tensor_x,tensor_y) # create your datset\n",
    "    my_dataloader = utils.DataLoader(my_dataset) # create your dataloader\n",
    "    return my_dataloader, my_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = load(train_data)\n",
    "test = load(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jrenugopal/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:27: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100000], Step [1/0], Loss: 641.9897\n",
      "Epoch [1/100000], Step [2/0], Loss: 930.7208\n",
      "Epoch [1/100000], Step [3/0], Loss: 1928.7993\n",
      "Epoch [1/100000], Step [4/0], Loss: 250.9016\n",
      "Epoch [1/100000], Step [5/0], Loss: 619.5741\n",
      "Epoch [1/100000], Step [6/0], Loss: 269.6597\n",
      "Epoch [1/100000], Step [7/0], Loss: 30.9277\n",
      "Epoch [1/100000], Step [8/0], Loss: 13.9926\n",
      "Epoch [1/100000], Step [9/0], Loss: 1.7848\n",
      "Epoch [1/100000], Step [10/0], Loss: 426.4815\n",
      "Epoch [2/100000], Step [1/0], Loss: 44.3120\n",
      "Epoch [2/100000], Step [2/0], Loss: 74.3031\n",
      "Epoch [2/100000], Step [3/0], Loss: 159.8012\n",
      "Epoch [2/100000], Step [4/0], Loss: 0.2689\n",
      "Epoch [2/100000], Step [5/0], Loss: 2.0232\n",
      "Epoch [2/100000], Step [6/0], Loss: 23.7683\n",
      "Epoch [2/100000], Step [7/0], Loss: 24.3890\n",
      "Epoch [2/100000], Step [8/0], Loss: 157.2764\n",
      "Epoch [2/100000], Step [9/0], Loss: 15.3711\n",
      "Epoch [2/100000], Step [10/0], Loss: 97.8407\n",
      "Epoch [3/100000], Step [1/0], Loss: 32.3496\n",
      "Epoch [3/100000], Step [2/0], Loss: 26.0769\n",
      "Epoch [3/100000], Step [3/0], Loss: 10.9226\n",
      "Epoch [3/100000], Step [4/0], Loss: 5.3458\n",
      "Epoch [3/100000], Step [5/0], Loss: 2.5603\n",
      "Epoch [3/100000], Step [6/0], Loss: 8.4635\n",
      "Epoch [3/100000], Step [7/0], Loss: 0.1212\n",
      "Epoch [3/100000], Step [8/0], Loss: 150.5070\n",
      "Epoch [3/100000], Step [9/0], Loss: 2.3536\n",
      "Epoch [3/100000], Step [10/0], Loss: 26.1785\n",
      "Epoch [4/100000], Step [1/0], Loss: 0.4479\n",
      "Epoch [4/100000], Step [2/0], Loss: 1.1172\n",
      "Epoch [4/100000], Step [3/0], Loss: 1.7063\n",
      "Epoch [4/100000], Step [4/0], Loss: 9.8718\n",
      "Epoch [4/100000], Step [5/0], Loss: 13.8268\n",
      "Epoch [4/100000], Step [6/0], Loss: 15.1684\n",
      "Epoch [4/100000], Step [7/0], Loss: 11.0654\n",
      "Epoch [4/100000], Step [8/0], Loss: 10.0702\n",
      "Epoch [4/100000], Step [9/0], Loss: 5.4801\n",
      "Epoch [4/100000], Step [10/0], Loss: 0.7318\n",
      "Epoch [5/100000], Step [1/0], Loss: 2.1386\n",
      "Epoch [5/100000], Step [2/0], Loss: 0.0053\n",
      "Epoch [5/100000], Step [3/0], Loss: 14.3814\n",
      "Epoch [5/100000], Step [4/0], Loss: 0.3848\n",
      "Epoch [5/100000], Step [5/0], Loss: 4.5309\n",
      "Epoch [5/100000], Step [6/0], Loss: 1.6365\n",
      "Epoch [5/100000], Step [7/0], Loss: 1.7504\n",
      "Epoch [5/100000], Step [8/0], Loss: 11.3911\n",
      "Epoch [5/100000], Step [9/0], Loss: 2.8844\n",
      "Epoch [5/100000], Step [10/0], Loss: 0.3699\n",
      "Epoch [6/100000], Step [1/0], Loss: 5.2457\n",
      "Epoch [6/100000], Step [2/0], Loss: 5.7787\n",
      "Epoch [6/100000], Step [3/0], Loss: 4.0574\n",
      "Epoch [6/100000], Step [4/0], Loss: 5.7547\n",
      "Epoch [6/100000], Step [5/0], Loss: 2.1374\n",
      "Epoch [6/100000], Step [6/0], Loss: 0.9776\n",
      "Epoch [6/100000], Step [7/0], Loss: 2.6678\n",
      "Epoch [6/100000], Step [8/0], Loss: 9.8072\n",
      "Epoch [6/100000], Step [9/0], Loss: 1.5470\n",
      "Epoch [6/100000], Step [10/0], Loss: 2.2005\n",
      "Epoch [7/100000], Step [1/0], Loss: 0.9458\n",
      "Epoch [7/100000], Step [2/0], Loss: 0.3349\n",
      "Epoch [7/100000], Step [3/0], Loss: 0.5890\n",
      "Epoch [7/100000], Step [4/0], Loss: 2.3469\n",
      "Epoch [7/100000], Step [5/0], Loss: 0.4050\n",
      "Epoch [7/100000], Step [6/0], Loss: 0.7911\n",
      "Epoch [7/100000], Step [7/0], Loss: 2.7301\n",
      "Epoch [7/100000], Step [8/0], Loss: 1.1272\n",
      "Epoch [7/100000], Step [9/0], Loss: 1.2674\n",
      "Epoch [7/100000], Step [10/0], Loss: 0.0005\n",
      "Epoch [8/100000], Step [1/0], Loss: 1.7163\n",
      "Epoch [8/100000], Step [2/0], Loss: 0.7617\n",
      "Epoch [8/100000], Step [3/0], Loss: 0.3198\n",
      "Epoch [8/100000], Step [4/0], Loss: 1.4801\n",
      "Epoch [8/100000], Step [5/0], Loss: 0.0000\n",
      "Epoch [8/100000], Step [6/0], Loss: 0.0142\n",
      "Epoch [8/100000], Step [7/0], Loss: 1.2673\n",
      "Epoch [8/100000], Step [8/0], Loss: 5.0746\n",
      "Epoch [8/100000], Step [9/0], Loss: 0.5050\n",
      "Epoch [8/100000], Step [10/0], Loss: 0.0056\n",
      "Epoch [9/100000], Step [1/0], Loss: 1.6702\n",
      "Epoch [9/100000], Step [2/0], Loss: 1.4659\n",
      "Epoch [9/100000], Step [3/0], Loss: 0.2401\n",
      "Epoch [9/100000], Step [4/0], Loss: 1.8438\n",
      "Epoch [9/100000], Step [5/0], Loss: 0.3193\n",
      "Epoch [9/100000], Step [6/0], Loss: 0.1624\n",
      "Epoch [9/100000], Step [7/0], Loss: 0.9335\n",
      "Epoch [9/100000], Step [8/0], Loss: 4.8289\n",
      "Epoch [9/100000], Step [9/0], Loss: 0.1225\n",
      "Epoch [9/100000], Step [10/0], Loss: 0.2073\n",
      "Epoch [10/100000], Step [1/0], Loss: 0.8548\n",
      "Epoch [10/100000], Step [2/0], Loss: 0.7172\n",
      "Epoch [10/100000], Step [3/0], Loss: 0.0360\n",
      "Epoch [10/100000], Step [4/0], Loss: 1.2194\n",
      "Epoch [10/100000], Step [5/0], Loss: 0.2556\n",
      "Epoch [10/100000], Step [6/0], Loss: 0.1958\n",
      "Epoch [10/100000], Step [7/0], Loss: 0.7112\n",
      "Epoch [10/100000], Step [8/0], Loss: 2.4094\n",
      "Epoch [10/100000], Step [9/0], Loss: 0.0116\n",
      "Epoch [10/100000], Step [10/0], Loss: 0.0545\n",
      "Epoch [11/100000], Step [1/0], Loss: 0.6411\n",
      "Epoch [11/100000], Step [2/0], Loss: 0.4946\n",
      "Epoch [11/100000], Step [3/0], Loss: 0.0027\n",
      "Epoch [11/100000], Step [4/0], Loss: 0.7155\n",
      "Epoch [11/100000], Step [5/0], Loss: 0.0670\n",
      "Epoch [11/100000], Step [6/0], Loss: 0.0493\n",
      "Epoch [11/100000], Step [7/0], Loss: 0.3678\n",
      "Epoch [11/100000], Step [8/0], Loss: 2.2816\n",
      "Epoch [11/100000], Step [9/0], Loss: 0.0166\n",
      "Epoch [11/100000], Step [10/0], Loss: 0.0003\n",
      "Epoch [12/100000], Step [1/0], Loss: 0.6560\n",
      "Epoch [12/100000], Step [2/0], Loss: 0.6610\n",
      "Epoch [12/100000], Step [3/0], Loss: 0.1487\n",
      "Epoch [12/100000], Step [4/0], Loss: 0.5704\n",
      "Epoch [12/100000], Step [5/0], Loss: 0.0669\n",
      "Epoch [12/100000], Step [6/0], Loss: 0.0163\n",
      "Epoch [12/100000], Step [7/0], Loss: 0.1663\n",
      "Epoch [12/100000], Step [8/0], Loss: 2.5228\n",
      "Epoch [12/100000], Step [9/0], Loss: 0.1085\n",
      "Epoch [12/100000], Step [10/0], Loss: 0.0127\n",
      "Epoch [13/100000], Step [1/0], Loss: 0.4742\n",
      "Epoch [13/100000], Step [2/0], Loss: 0.5710\n",
      "Epoch [13/100000], Step [3/0], Loss: 0.2933\n",
      "Epoch [13/100000], Step [4/0], Loss: 0.4600\n",
      "Epoch [13/100000], Step [5/0], Loss: 0.1164\n",
      "Epoch [13/100000], Step [6/0], Loss: 0.0226\n",
      "Epoch [13/100000], Step [7/0], Loss: 0.0874\n",
      "Epoch [13/100000], Step [8/0], Loss: 2.0766\n",
      "Epoch [13/100000], Step [9/0], Loss: 0.2064\n",
      "Epoch [13/100000], Step [10/0], Loss: 0.0194\n",
      "Epoch [14/100000], Step [1/0], Loss: 0.2844\n",
      "Epoch [14/100000], Step [2/0], Loss: 0.4265\n",
      "Epoch [14/100000], Step [3/0], Loss: 0.2690\n",
      "Epoch [14/100000], Step [4/0], Loss: 0.3167\n",
      "Epoch [14/100000], Step [5/0], Loss: 0.1005\n",
      "Epoch [14/100000], Step [6/0], Loss: 0.0174\n",
      "Epoch [14/100000], Step [7/0], Loss: 0.0430\n",
      "Epoch [14/100000], Step [8/0], Loss: 1.5723\n",
      "Epoch [14/100000], Step [9/0], Loss: 0.2678\n",
      "Epoch [14/100000], Step [10/0], Loss: 0.0067\n",
      "Epoch [15/100000], Step [1/0], Loss: 0.2054\n",
      "Epoch [15/100000], Step [2/0], Loss: 0.3370\n",
      "Epoch [15/100000], Step [3/0], Loss: 0.2458\n",
      "Epoch [15/100000], Step [4/0], Loss: 0.2085\n",
      "Epoch [15/100000], Step [5/0], Loss: 0.0592\n",
      "Epoch [15/100000], Step [6/0], Loss: 0.0050\n",
      "Epoch [15/100000], Step [7/0], Loss: 0.0165\n",
      "Epoch [15/100000], Step [8/0], Loss: 1.2970\n",
      "Epoch [15/100000], Step [9/0], Loss: 0.2944\n",
      "Epoch [15/100000], Step [10/0], Loss: 0.0000\n",
      "Epoch [16/100000], Step [1/0], Loss: 0.1683\n",
      "Epoch [16/100000], Step [2/0], Loss: 0.3048\n",
      "Epoch [16/100000], Step [3/0], Loss: 0.2764\n",
      "Epoch [16/100000], Step [4/0], Loss: 0.1479\n",
      "Epoch [16/100000], Step [5/0], Loss: 0.0338\n",
      "Epoch [16/100000], Step [6/0], Loss: 0.0001\n",
      "Epoch [16/100000], Step [7/0], Loss: 0.0035\n",
      "Epoch [16/100000], Step [8/0], Loss: 1.2038\n",
      "Epoch [16/100000], Step [9/0], Loss: 0.3001\n",
      "Epoch [16/100000], Step [10/0], Loss: 0.0008\n",
      "Epoch [17/100000], Step [1/0], Loss: 0.1454\n",
      "Epoch [17/100000], Step [2/0], Loss: 0.2988\n",
      "Epoch [17/100000], Step [3/0], Loss: 0.3551\n",
      "Epoch [17/100000], Step [4/0], Loss: 0.1139\n",
      "Epoch [17/100000], Step [5/0], Loss: 0.0223\n",
      "Epoch [17/100000], Step [6/0], Loss: 0.0018\n",
      "Epoch [17/100000], Step [7/0], Loss: 0.0000\n",
      "Epoch [17/100000], Step [8/0], Loss: 1.1933\n",
      "Epoch [17/100000], Step [9/0], Loss: 0.2895\n",
      "Epoch [17/100000], Step [10/0], Loss: 0.0018\n",
      "Epoch [18/100000], Step [1/0], Loss: 0.1287\n",
      "Epoch [18/100000], Step [2/0], Loss: 0.3010\n",
      "Epoch [18/100000], Step [3/0], Loss: 0.4311\n",
      "Epoch [18/100000], Step [4/0], Loss: 0.0974\n",
      "Epoch [18/100000], Step [5/0], Loss: 0.0191\n",
      "Epoch [18/100000], Step [6/0], Loss: 0.0052\n",
      "Epoch [18/100000], Step [7/0], Loss: 0.0017\n",
      "Epoch [18/100000], Step [8/0], Loss: 1.1986\n",
      "Epoch [18/100000], Step [9/0], Loss: 0.2687\n",
      "Epoch [18/100000], Step [10/0], Loss: 0.0032\n",
      "Epoch [19/100000], Step [1/0], Loss: 0.1160\n",
      "Epoch [19/100000], Step [2/0], Loss: 0.3019\n",
      "Epoch [19/100000], Step [3/0], Loss: 0.4980\n",
      "Epoch [19/100000], Step [4/0], Loss: 0.0824\n",
      "Epoch [19/100000], Step [5/0], Loss: 0.0153\n",
      "Epoch [19/100000], Step [6/0], Loss: 0.0105\n",
      "Epoch [19/100000], Step [7/0], Loss: 0.0061\n",
      "Epoch [19/100000], Step [8/0], Loss: 1.2387\n",
      "Epoch [19/100000], Step [9/0], Loss: 0.2439\n",
      "Epoch [19/100000], Step [10/0], Loss: 0.0080\n",
      "Epoch [20/100000], Step [1/0], Loss: 0.1116\n",
      "Epoch [20/100000], Step [2/0], Loss: 0.3135\n",
      "Epoch [20/100000], Step [3/0], Loss: 0.5743\n",
      "Epoch [20/100000], Step [4/0], Loss: 0.0693\n",
      "Epoch [20/100000], Step [5/0], Loss: 0.0097\n",
      "Epoch [20/100000], Step [6/0], Loss: 0.0217\n",
      "Epoch [20/100000], Step [7/0], Loss: 0.0133\n",
      "Epoch [20/100000], Step [8/0], Loss: 1.3119\n",
      "Epoch [20/100000], Step [9/0], Loss: 0.2184\n",
      "Epoch [20/100000], Step [10/0], Loss: 0.0180\n",
      "Epoch [21/100000], Step [1/0], Loss: 0.1144\n",
      "Epoch [21/100000], Step [2/0], Loss: 0.3395\n",
      "Epoch [21/100000], Step [3/0], Loss: 0.6758\n",
      "Epoch [21/100000], Step [4/0], Loss: 0.0597\n",
      "Epoch [21/100000], Step [5/0], Loss: 0.0045\n",
      "Epoch [21/100000], Step [6/0], Loss: 0.0414\n",
      "Epoch [21/100000], Step [7/0], Loss: 0.0235\n",
      "Epoch [21/100000], Step [8/0], Loss: 1.4345\n",
      "Epoch [21/100000], Step [9/0], Loss: 0.1939\n",
      "Epoch [21/100000], Step [10/0], Loss: 0.0354\n",
      "Epoch [22/100000], Step [1/0], Loss: 0.1238\n",
      "Epoch [22/100000], Step [2/0], Loss: 0.3819\n",
      "Epoch [22/100000], Step [3/0], Loss: 0.8118\n",
      "Epoch [22/100000], Step [4/0], Loss: 0.0532\n",
      "Epoch [22/100000], Step [5/0], Loss: 0.0009\n",
      "Epoch [22/100000], Step [6/0], Loss: 0.0721\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22/100000], Step [7/0], Loss: 0.0374\n",
      "Epoch [22/100000], Step [8/0], Loss: 1.6161\n",
      "Epoch [22/100000], Step [9/0], Loss: 0.1722\n",
      "Epoch [22/100000], Step [10/0], Loss: 0.0632\n",
      "Epoch [23/100000], Step [1/0], Loss: 0.1410\n",
      "Epoch [23/100000], Step [2/0], Loss: 0.4434\n",
      "Epoch [23/100000], Step [3/0], Loss: 0.9932\n",
      "Epoch [23/100000], Step [4/0], Loss: 0.0469\n",
      "Epoch [23/100000], Step [5/0], Loss: 0.0001\n",
      "Epoch [23/100000], Step [6/0], Loss: 0.1171\n",
      "Epoch [23/100000], Step [7/0], Loss: 0.0590\n",
      "Epoch [23/100000], Step [8/0], Loss: 1.8580\n",
      "Epoch [23/100000], Step [9/0], Loss: 0.1596\n",
      "Epoch [23/100000], Step [10/0], Loss: 0.1475\n",
      "Epoch [24/100000], Step [1/0], Loss: 0.1543\n",
      "Epoch [24/100000], Step [2/0], Loss: 0.4800\n",
      "Epoch [24/100000], Step [3/0], Loss: 1.0854\n",
      "Epoch [24/100000], Step [4/0], Loss: 0.0367\n",
      "Epoch [24/100000], Step [5/0], Loss: 0.0046\n",
      "Epoch [24/100000], Step [6/0], Loss: 0.1697\n",
      "Epoch [24/100000], Step [7/0], Loss: 0.0772\n",
      "Epoch [24/100000], Step [8/0], Loss: 1.9904\n",
      "Epoch [24/100000], Step [9/0], Loss: 0.1404\n",
      "Epoch [24/100000], Step [10/0], Loss: 0.1657\n",
      "Epoch [25/100000], Step [1/0], Loss: 0.1885\n",
      "Epoch [25/100000], Step [2/0], Loss: 0.5942\n",
      "Epoch [25/100000], Step [3/0], Loss: 1.3903\n",
      "Epoch [25/100000], Step [4/0], Loss: 0.0384\n",
      "Epoch [25/100000], Step [5/0], Loss: 0.0157\n",
      "Epoch [25/100000], Step [6/0], Loss: 0.2564\n",
      "Epoch [25/100000], Step [7/0], Loss: 0.1034\n",
      "Epoch [25/100000], Step [8/0], Loss: 2.3826\n",
      "Epoch [25/100000], Step [9/0], Loss: 0.1216\n",
      "Epoch [25/100000], Step [10/0], Loss: 0.2983\n",
      "Epoch [26/100000], Step [1/0], Loss: 0.2611\n",
      "Epoch [26/100000], Step [2/0], Loss: 0.7588\n",
      "Epoch [26/100000], Step [3/0], Loss: 1.6832\n",
      "Epoch [26/100000], Step [4/0], Loss: 0.0308\n",
      "Epoch [26/100000], Step [5/0], Loss: 0.0603\n",
      "Epoch [26/100000], Step [6/0], Loss: 0.4279\n",
      "Epoch [26/100000], Step [7/0], Loss: 0.1549\n",
      "Epoch [26/100000], Step [8/0], Loss: 2.9551\n",
      "Epoch [26/100000], Step [9/0], Loss: 0.1124\n",
      "Epoch [26/100000], Step [10/0], Loss: 0.4878\n",
      "Epoch [27/100000], Step [1/0], Loss: 0.3490\n",
      "Epoch [27/100000], Step [2/0], Loss: 0.9984\n",
      "Epoch [27/100000], Step [3/0], Loss: 2.2479\n",
      "Epoch [27/100000], Step [4/0], Loss: 0.0306\n",
      "Epoch [27/100000], Step [5/0], Loss: 0.1075\n",
      "Epoch [27/100000], Step [6/0], Loss: 0.6274\n",
      "Epoch [27/100000], Step [7/0], Loss: 0.2210\n",
      "Epoch [27/100000], Step [8/0], Loss: 3.7640\n",
      "Epoch [27/100000], Step [9/0], Loss: 0.1038\n",
      "Epoch [27/100000], Step [10/0], Loss: 0.7054\n",
      "Epoch [28/100000], Step [1/0], Loss: 0.4538\n",
      "Epoch [28/100000], Step [2/0], Loss: 1.2951\n",
      "Epoch [28/100000], Step [3/0], Loss: 2.9693\n",
      "Epoch [28/100000], Step [4/0], Loss: 0.0321\n",
      "Epoch [28/100000], Step [5/0], Loss: 0.1671\n",
      "Epoch [28/100000], Step [6/0], Loss: 0.8764\n",
      "Epoch [28/100000], Step [7/0], Loss: 0.2904\n",
      "Epoch [28/100000], Step [8/0], Loss: 4.7760\n",
      "Epoch [28/100000], Step [9/0], Loss: 0.0923\n",
      "Epoch [28/100000], Step [10/0], Loss: 0.9822\n",
      "Epoch [29/100000], Step [1/0], Loss: 0.5843\n",
      "Epoch [29/100000], Step [2/0], Loss: 1.6617\n",
      "Epoch [29/100000], Step [3/0], Loss: 3.8588\n",
      "Epoch [29/100000], Step [4/0], Loss: 0.0359\n",
      "Epoch [29/100000], Step [5/0], Loss: 0.2467\n",
      "Epoch [29/100000], Step [6/0], Loss: 1.1939\n",
      "Epoch [29/100000], Step [7/0], Loss: 0.3916\n",
      "Epoch [29/100000], Step [8/0], Loss: 5.9973\n",
      "Epoch [29/100000], Step [9/0], Loss: 0.0854\n",
      "Epoch [29/100000], Step [10/0], Loss: 1.3488\n",
      "Epoch [30/100000], Step [1/0], Loss: 0.7480\n",
      "Epoch [30/100000], Step [2/0], Loss: 2.1185\n",
      "Epoch [30/100000], Step [3/0], Loss: 4.9596\n",
      "Epoch [30/100000], Step [4/0], Loss: 0.0381\n",
      "Epoch [30/100000], Step [5/0], Loss: 0.3486\n",
      "Epoch [30/100000], Step [6/0], Loss: 1.5915\n",
      "Epoch [30/100000], Step [7/0], Loss: 0.5078\n",
      "Epoch [30/100000], Step [8/0], Loss: 7.4887\n",
      "Epoch [30/100000], Step [9/0], Loss: 0.0747\n",
      "Epoch [30/100000], Step [10/0], Loss: 1.8029\n",
      "Epoch [31/100000], Step [1/0], Loss: 0.9521\n",
      "Epoch [31/100000], Step [2/0], Loss: 2.6754\n",
      "Epoch [31/100000], Step [3/0], Loss: 6.3135\n",
      "Epoch [31/100000], Step [4/0], Loss: 0.0410\n",
      "Epoch [31/100000], Step [5/0], Loss: 0.4665\n",
      "Epoch [31/100000], Step [6/0], Loss: 2.0634\n",
      "Epoch [31/100000], Step [7/0], Loss: 0.6800\n",
      "Epoch [31/100000], Step [8/0], Loss: 9.2614\n",
      "Epoch [31/100000], Step [9/0], Loss: 0.0715\n",
      "Epoch [31/100000], Step [10/0], Loss: 2.3375\n",
      "Epoch [32/100000], Step [1/0], Loss: 1.1902\n",
      "Epoch [32/100000], Step [2/0], Loss: 3.3486\n",
      "Epoch [32/100000], Step [3/0], Loss: 7.9708\n",
      "Epoch [32/100000], Step [4/0], Loss: 0.0495\n",
      "Epoch [32/100000], Step [5/0], Loss: 0.5834\n",
      "Epoch [32/100000], Step [6/0], Loss: 2.5999\n",
      "Epoch [32/100000], Step [7/0], Loss: 0.8413\n",
      "Epoch [32/100000], Step [8/0], Loss: 11.4808\n",
      "Epoch [32/100000], Step [9/0], Loss: 0.0604\n",
      "Epoch [32/100000], Step [10/0], Loss: 2.9139\n",
      "Epoch [33/100000], Step [1/0], Loss: 1.4612\n",
      "Epoch [33/100000], Step [2/0], Loss: 4.1398\n",
      "Epoch [33/100000], Step [3/0], Loss: 10.0161\n",
      "Epoch [33/100000], Step [4/0], Loss: 0.0868\n",
      "Epoch [33/100000], Step [5/0], Loss: 0.6975\n",
      "Epoch [33/100000], Step [6/0], Loss: 3.2183\n",
      "Epoch [33/100000], Step [7/0], Loss: 0.9658\n",
      "Epoch [33/100000], Step [8/0], Loss: 14.2889\n",
      "Epoch [33/100000], Step [9/0], Loss: 0.0469\n",
      "Epoch [33/100000], Step [10/0], Loss: 3.6149\n",
      "Epoch [34/100000], Step [1/0], Loss: 1.9552\n",
      "Epoch [34/100000], Step [2/0], Loss: 5.0785\n",
      "Epoch [34/100000], Step [3/0], Loss: 12.1376\n",
      "Epoch [34/100000], Step [4/0], Loss: 0.0730\n",
      "Epoch [34/100000], Step [5/0], Loss: 0.8252\n",
      "Epoch [34/100000], Step [6/0], Loss: 3.8457\n",
      "Epoch [34/100000], Step [7/0], Loss: 1.3592\n",
      "Epoch [34/100000], Step [8/0], Loss: 16.6003\n",
      "Epoch [34/100000], Step [9/0], Loss: 0.0864\n",
      "Epoch [34/100000], Step [10/0], Loss: 4.1241\n",
      "Epoch [35/100000], Step [1/0], Loss: 2.0103\n",
      "Epoch [35/100000], Step [2/0], Loss: 5.9254\n",
      "Epoch [35/100000], Step [3/0], Loss: 15.1244\n",
      "Epoch [35/100000], Step [4/0], Loss: 0.1012\n",
      "Epoch [35/100000], Step [5/0], Loss: 0.7787\n",
      "Epoch [35/100000], Step [6/0], Loss: 4.2900\n",
      "Epoch [35/100000], Step [7/0], Loss: 1.6046\n",
      "Epoch [35/100000], Step [8/0], Loss: 20.0339\n",
      "Epoch [35/100000], Step [9/0], Loss: 0.1161\n",
      "Epoch [35/100000], Step [10/0], Loss: 4.3889\n",
      "Epoch [36/100000], Step [1/0], Loss: 2.2485\n",
      "Epoch [36/100000], Step [2/0], Loss: 6.8736\n",
      "Epoch [36/100000], Step [3/0], Loss: 18.2462\n",
      "Epoch [36/100000], Step [4/0], Loss: 0.1564\n",
      "Epoch [36/100000], Step [5/0], Loss: 0.6546\n",
      "Epoch [36/100000], Step [6/0], Loss: 4.5523\n",
      "Epoch [36/100000], Step [7/0], Loss: 1.8218\n",
      "Epoch [36/100000], Step [8/0], Loss: 23.6028\n",
      "Epoch [36/100000], Step [9/0], Loss: 0.1296\n",
      "Epoch [36/100000], Step [10/0], Loss: 4.3583\n",
      "Epoch [37/100000], Step [1/0], Loss: 2.4032\n",
      "Epoch [37/100000], Step [2/0], Loss: 7.6846\n",
      "Epoch [37/100000], Step [3/0], Loss: 21.2616\n",
      "Epoch [37/100000], Step [4/0], Loss: 0.2291\n",
      "Epoch [37/100000], Step [5/0], Loss: 0.4624\n",
      "Epoch [37/100000], Step [6/0], Loss: 4.5399\n",
      "Epoch [37/100000], Step [7/0], Loss: 2.0072\n",
      "Epoch [37/100000], Step [8/0], Loss: 26.6483\n",
      "Epoch [37/100000], Step [9/0], Loss: 0.1473\n",
      "Epoch [37/100000], Step [10/0], Loss: 3.9462\n",
      "Epoch [38/100000], Step [1/0], Loss: 2.3482\n",
      "Epoch [38/100000], Step [2/0], Loss: 8.0774\n",
      "Epoch [38/100000], Step [3/0], Loss: 23.6691\n",
      "Epoch [38/100000], Step [4/0], Loss: 0.3012\n",
      "Epoch [38/100000], Step [5/0], Loss: 0.2478\n",
      "Epoch [38/100000], Step [6/0], Loss: 4.1871\n",
      "Epoch [38/100000], Step [7/0], Loss: 2.1035\n",
      "Epoch [38/100000], Step [8/0], Loss: 28.1870\n",
      "Epoch [38/100000], Step [9/0], Loss: 0.1676\n",
      "Epoch [38/100000], Step [10/0], Loss: 3.1653\n",
      "Epoch [39/100000], Step [1/0], Loss: 2.0485\n",
      "Epoch [39/100000], Step [2/0], Loss: 7.8209\n",
      "Epoch [39/100000], Step [3/0], Loss: 24.6256\n",
      "Epoch [39/100000], Step [4/0], Loss: 0.3401\n",
      "Epoch [39/100000], Step [5/0], Loss: 0.0887\n",
      "Epoch [39/100000], Step [6/0], Loss: 3.5612\n",
      "Epoch [39/100000], Step [7/0], Loss: 2.0781\n",
      "Epoch [39/100000], Step [8/0], Loss: 27.3441\n",
      "Epoch [39/100000], Step [9/0], Loss: 0.1830\n",
      "Epoch [39/100000], Step [10/0], Loss: 2.1627\n",
      "Epoch [40/100000], Step [1/0], Loss: 1.5618\n",
      "Epoch [40/100000], Step [2/0], Loss: 6.8550\n",
      "Epoch [40/100000], Step [3/0], Loss: 23.4583\n",
      "Epoch [40/100000], Step [4/0], Loss: 0.3369\n",
      "Epoch [40/100000], Step [5/0], Loss: 0.0095\n",
      "Epoch [40/100000], Step [6/0], Loss: 2.7295\n",
      "Epoch [40/100000], Step [7/0], Loss: 1.8919\n",
      "Epoch [40/100000], Step [8/0], Loss: 23.8738\n",
      "Epoch [40/100000], Step [9/0], Loss: 0.1819\n",
      "Epoch [40/100000], Step [10/0], Loss: 1.2167\n",
      "Epoch [41/100000], Step [1/0], Loss: 1.0030\n",
      "Epoch [41/100000], Step [2/0], Loss: 5.3407\n",
      "Epoch [41/100000], Step [3/0], Loss: 20.1291\n",
      "Epoch [41/100000], Step [4/0], Loss: 0.2773\n",
      "Epoch [41/100000], Step [5/0], Loss: 0.0035\n",
      "Epoch [41/100000], Step [6/0], Loss: 1.9041\n",
      "Epoch [41/100000], Step [7/0], Loss: 1.5839\n",
      "Epoch [41/100000], Step [8/0], Loss: 18.4087\n",
      "Epoch [41/100000], Step [9/0], Loss: 0.1579\n",
      "Epoch [41/100000], Step [10/0], Loss: 0.5617\n",
      "Epoch [42/100000], Step [1/0], Loss: 0.5165\n",
      "Epoch [42/100000], Step [2/0], Loss: 3.6218\n",
      "Epoch [42/100000], Step [3/0], Loss: 15.4080\n",
      "Epoch [42/100000], Step [4/0], Loss: 0.1755\n",
      "Epoch [42/100000], Step [5/0], Loss: 0.0208\n",
      "Epoch [42/100000], Step [6/0], Loss: 1.2319\n",
      "Epoch [42/100000], Step [7/0], Loss: 1.2259\n",
      "Epoch [42/100000], Step [8/0], Loss: 12.3475\n",
      "Epoch [42/100000], Step [9/0], Loss: 0.1126\n",
      "Epoch [42/100000], Step [10/0], Loss: 0.1978\n",
      "Epoch [43/100000], Step [1/0], Loss: 0.1955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [43/100000], Step [2/0], Loss: 2.1394\n",
      "Epoch [43/100000], Step [3/0], Loss: 10.6185\n",
      "Epoch [43/100000], Step [4/0], Loss: 0.0816\n",
      "Epoch [43/100000], Step [5/0], Loss: 0.0289\n",
      "Epoch [43/100000], Step [6/0], Loss: 0.7493\n",
      "Epoch [43/100000], Step [7/0], Loss: 0.8880\n",
      "Epoch [43/100000], Step [8/0], Loss: 7.2357\n",
      "Epoch [43/100000], Step [9/0], Loss: 0.0609\n",
      "Epoch [43/100000], Step [10/0], Loss: 0.0505\n",
      "Epoch [44/100000], Step [1/0], Loss: 0.0418\n",
      "Epoch [44/100000], Step [2/0], Loss: 1.0779\n",
      "Epoch [44/100000], Step [3/0], Loss: 6.5653\n",
      "Epoch [44/100000], Step [4/0], Loss: 0.0243\n",
      "Epoch [44/100000], Step [5/0], Loss: 0.0261\n",
      "Epoch [44/100000], Step [6/0], Loss: 0.4189\n",
      "Epoch [44/100000], Step [7/0], Loss: 0.5918\n",
      "Epoch [44/100000], Step [8/0], Loss: 3.6201\n",
      "Epoch [44/100000], Step [9/0], Loss: 0.0198\n",
      "Epoch [44/100000], Step [10/0], Loss: 0.0048\n",
      "Epoch [45/100000], Step [1/0], Loss: 0.0003\n",
      "Epoch [45/100000], Step [2/0], Loss: 0.4356\n",
      "Epoch [45/100000], Step [3/0], Loss: 3.6052\n",
      "Epoch [45/100000], Step [4/0], Loss: 0.0023\n",
      "Epoch [45/100000], Step [5/0], Loss: 0.0207\n",
      "Epoch [45/100000], Step [6/0], Loss: 0.2054\n",
      "Epoch [45/100000], Step [7/0], Loss: 0.3560\n",
      "Epoch [45/100000], Step [8/0], Loss: 1.4482\n",
      "Epoch [45/100000], Step [9/0], Loss: 0.0011\n",
      "Epoch [45/100000], Step [10/0], Loss: 0.0019\n",
      "Epoch [46/100000], Step [1/0], Loss: 0.0162\n",
      "Epoch [46/100000], Step [2/0], Loss: 0.1128\n",
      "Epoch [46/100000], Step [3/0], Loss: 1.6966\n",
      "Epoch [46/100000], Step [4/0], Loss: 0.0011\n",
      "Epoch [46/100000], Step [5/0], Loss: 0.0174\n",
      "Epoch [46/100000], Step [6/0], Loss: 0.0775\n",
      "Epoch [46/100000], Step [7/0], Loss: 0.1854\n",
      "Epoch [46/100000], Step [8/0], Loss: 0.3665\n",
      "Epoch [46/100000], Step [9/0], Loss: 0.0041\n",
      "Epoch [46/100000], Step [10/0], Loss: 0.0190\n",
      "Epoch [47/100000], Step [1/0], Loss: 0.0557\n",
      "Epoch [47/100000], Step [2/0], Loss: 0.0037\n",
      "Epoch [47/100000], Step [3/0], Loss: 0.6127\n",
      "Epoch [47/100000], Step [4/0], Loss: 0.0073\n",
      "Epoch [47/100000], Step [5/0], Loss: 0.0187\n",
      "Epoch [47/100000], Step [6/0], Loss: 0.0125\n",
      "Epoch [47/100000], Step [7/0], Loss: 0.0745\n",
      "Epoch [47/100000], Step [8/0], Loss: 0.0105\n",
      "Epoch [47/100000], Step [9/0], Loss: 0.0199\n",
      "Epoch [47/100000], Step [10/0], Loss: 0.0530\n",
      "Epoch [48/100000], Step [1/0], Loss: 0.1029\n",
      "Epoch [48/100000], Step [2/0], Loss: 0.0303\n",
      "Epoch [48/100000], Step [3/0], Loss: 0.1095\n",
      "Epoch [48/100000], Step [4/0], Loss: 0.0127\n",
      "Epoch [48/100000], Step [5/0], Loss: 0.0280\n",
      "Epoch [48/100000], Step [6/0], Loss: 0.0027\n",
      "Epoch [48/100000], Step [7/0], Loss: 0.0156\n",
      "Epoch [48/100000], Step [8/0], Loss: 0.1016\n",
      "Epoch [48/100000], Step [9/0], Loss: 0.0383\n",
      "Epoch [48/100000], Step [10/0], Loss: 0.1085\n",
      "Epoch [49/100000], Step [1/0], Loss: 0.1539\n",
      "Epoch [49/100000], Step [2/0], Loss: 0.1437\n",
      "Epoch [49/100000], Step [3/0], Loss: 0.0034\n",
      "Epoch [49/100000], Step [4/0], Loss: 0.0150\n",
      "Epoch [49/100000], Step [5/0], Loss: 0.0474\n",
      "Epoch [49/100000], Step [6/0], Loss: 0.0464\n",
      "Epoch [49/100000], Step [7/0], Loss: 0.0002\n",
      "Epoch [49/100000], Step [8/0], Loss: 0.4694\n",
      "Epoch [49/100000], Step [9/0], Loss: 0.0531\n",
      "Epoch [49/100000], Step [10/0], Loss: 0.1981\n",
      "Epoch [50/100000], Step [1/0], Loss: 0.2131\n",
      "Epoch [50/100000], Step [2/0], Loss: 0.3139\n",
      "Epoch [50/100000], Step [3/0], Loss: 0.1662\n",
      "Epoch [50/100000], Step [4/0], Loss: 0.0140\n",
      "Epoch [50/100000], Step [5/0], Loss: 0.0840\n",
      "Epoch [50/100000], Step [6/0], Loss: 0.1483\n",
      "Epoch [50/100000], Step [7/0], Loss: 0.0218\n",
      "Epoch [50/100000], Step [8/0], Loss: 1.0320\n",
      "Epoch [50/100000], Step [9/0], Loss: 0.0634\n",
      "Epoch [50/100000], Step [10/0], Loss: 0.3322\n"
     ]
    }
   ],
   "source": [
    "input_size = 1       # The image size = 28 x 28 = 784\n",
    "hidden_size = 500      # The number of nodes at the hidden layer\n",
    "num_classes = 1       # The number of output classes. In this case, from 0 to 9\n",
    "num_epochs = 100000         # The number of times entire dataset is trained\n",
    "batch_size = 100       # The size of input data took for one iteration\n",
    "learning_rate = 0.001  # The speed of convergence\n",
    "net = Net(input_size, hidden_size, num_classes)\n",
    "\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "for epoch in range(50):\n",
    "    for i, (x, y) in enumerate(train):   # Load a batch of images with its (index, data, class)\n",
    "        x = Variable(x.float())         # Convert torch tensor to Variable: change image from a vector of size 784 to a matrix of 28 x 28\n",
    "        y = Variable(y.float())\n",
    "        \n",
    "        optimizer.zero_grad()                             # Intialize the hidden weight to all zeros\n",
    "        outputs = net(x)                             # Forward pass: compute the output class given a image\n",
    "        loss = criterion(outputs, y)                 # Compute the loss: difference between the output class and the pre-given label\n",
    "        loss.backward()                                   # Backward pass: compute the weight\n",
    "        optimizer.step()                                  # Optimizer: update the weights of hidden nodes\n",
    "        \n",
    "        \n",
    "        print('Epoch [%d/%d], Step [%d/%d], Loss: %.4f'\n",
    "                 %(epoch+1, num_epochs, i+1, len(train_data)//batch_size, loss.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0508, grad_fn=<MseLossBackward>) tensor([13]) tensor([13.2254], grad_fn=<ThAddBackward>)\n",
      "tensor(0.8205, grad_fn=<MseLossBackward>) tensor([57]) tensor([57.9058], grad_fn=<ThAddBackward>)\n",
      "tensor(0.3738, grad_fn=<MseLossBackward>) tensor([29]) tensor([29.6114], grad_fn=<ThAddBackward>)\n",
      "tensor(0.6180, grad_fn=<MseLossBackward>) tensor([45]) tensor([45.7862], grad_fn=<ThAddBackward>)\n",
      "tensor(0.0413, grad_fn=<MseLossBackward>) tensor([5]) tensor([5.2032], grad_fn=<ThAddBackward>)\n"
     ]
    }
   ],
   "source": [
    "for x,y in test:\n",
    "    x = Variable(x.float())\n",
    "    outputs = net(x.float())\n",
    "    loss = criterion(outputs, y.float())\n",
    "    print(loss, y, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWords(file, model):\n",
    "    source, target = [],[]\n",
    "    with open(file) as f:\n",
    "        for line in f:\n",
    "            s, t = line.split()\n",
    "            #if s.split(\"|\")[0] in model and t.split(\"|\")[0] in model:\n",
    "            source.append(model[s][1])\n",
    "            target.append(model[t][1])\n",
    "             #   print(s.split(\"|\")[0])\n",
    "             #   source.append(model[s.split(\"|\")[0]])\n",
    "              #  target.append(model[t.split(\"|\")[0]])\n",
    "    return source, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "source, target = getWords(\"/home/jrenugopal/guys-NLP/nouns_plural/verbs.txt\", s2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "source, target = np.array(source),np.array(target) \n",
    "_,full_dataset = convToDataloader(source, target)\n",
    "train, test = torch.utils.data.random_split(full_dataset, [int(0.8*len(source)), len(source)-int(0.8*len(source))])\n",
    "train,test = load(train),load(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "tensor(0.9802, grad_fn=<DivBackward0>) tensor(0.3338)\n",
      "tensor(0.3502, grad_fn=<DivBackward0>) tensor(0.2219)\n",
      "tensor(0.2287, grad_fn=<DivBackward0>) tensor(0.1263)\n",
      "tensor(0.3253, grad_fn=<DivBackward0>) tensor(0.2887)\n",
      "tensor(0.2518, grad_fn=<DivBackward0>) tensor(0.1248)\n",
      "tensor(0.2697, grad_fn=<DivBackward0>) tensor(0.2323)\n",
      "1\n",
      "tensor(0.3103, grad_fn=<DivBackward0>) tensor(0.3338)\n",
      "tensor(0.3042, grad_fn=<DivBackward0>) tensor(0.2219)\n",
      "tensor(0.1429, grad_fn=<DivBackward0>) tensor(0.1263)\n",
      "tensor(0.2990, grad_fn=<DivBackward0>) tensor(0.2887)\n",
      "tensor(0.1858, grad_fn=<DivBackward0>) tensor(0.1248)\n",
      "tensor(0.2481, grad_fn=<DivBackward0>) tensor(0.2323)\n",
      "2\n",
      "tensor(0.3004, grad_fn=<DivBackward0>) tensor(0.3338)\n",
      "tensor(0.2960, grad_fn=<DivBackward0>) tensor(0.2219)\n",
      "tensor(0.1218, grad_fn=<DivBackward0>) tensor(0.1263)\n",
      "tensor(0.2772, grad_fn=<DivBackward0>) tensor(0.2887)\n",
      "tensor(0.1446, grad_fn=<DivBackward0>) tensor(0.1248)\n",
      "tensor(0.2427, grad_fn=<DivBackward0>) tensor(0.2323)\n",
      "3\n",
      "tensor(0.2951, grad_fn=<DivBackward0>) tensor(0.3338)\n",
      "tensor(0.2699, grad_fn=<DivBackward0>) tensor(0.2219)\n",
      "tensor(0.1126, grad_fn=<DivBackward0>) tensor(0.1263)\n",
      "tensor(0.2921, grad_fn=<DivBackward0>) tensor(0.2887)\n",
      "tensor(0.1446, grad_fn=<DivBackward0>) tensor(0.1248)\n",
      "tensor(0.2392, grad_fn=<DivBackward0>) tensor(0.2323)\n",
      "4\n",
      "tensor(0.2913, grad_fn=<DivBackward0>) tensor(0.3338)\n",
      "tensor(0.2525, grad_fn=<DivBackward0>) tensor(0.2219)\n",
      "tensor(0.1088, grad_fn=<DivBackward0>) tensor(0.1263)\n",
      "tensor(0.2794, grad_fn=<DivBackward0>) tensor(0.2887)\n",
      "tensor(0.1332, grad_fn=<DivBackward0>) tensor(0.1248)\n",
      "tensor(0.2270, grad_fn=<DivBackward0>) tensor(0.2323)\n"
     ]
    }
   ],
   "source": [
    "input_size = 128       # The image size = 28 x 28 = 784\n",
    "hidden_size = 128      # The number of nodes at the hidden layer\n",
    "num_classes = 128       # The number of output classes. In this case, from 0 to 9\n",
    "num_epochs = 5         # The number of times entire dataset is trained\n",
    "batch_size = 100       # The size of input data took for one iteration\n",
    "learning_rate = 0.001  # The speed of convergence\n",
    "net = Net(input_size, hidden_size, num_classes)\n",
    "#,torch.tensor([[1.]])\n",
    "\n",
    "criterion = torch.nn.CosineEmbeddingLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(epoch)\n",
    "    for i, (x, y) in enumerate(train):   # Load a batch of images with its (index, data, class)\n",
    "        x = Variable(x.float())         # Convert torch tensor to Variable: change image from a vector of size 784 to a matrix of 28 x 28\n",
    "        y = Variable(y.float())\n",
    "        optimizer.zero_grad()                             # Intialize the hidden weight to all zeros\n",
    "        outputs = net(x)                             # Forward pass: compute the output class given a image\n",
    "        loss = criterion(outputs, y,torch.tensor([[1.]]))                 # Compute the loss: difference between the output class and the pre-given label\n",
    "        if i%1000 is 0:\n",
    "            print(loss, criterion(x, y,torch.tensor([[1.]])))\n",
    "        loss.backward()                                   # Backward pass: compute the weight\n",
    "        optimizer.step()                                  # Optimizer: update the weights of hidden nodes\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.171071953010279\n",
      "0.7100856372438339\n",
      "14.027268831381539\n",
      "0.04195851342960588\n"
     ]
    }
   ],
   "source": [
    "#Calculate the accuracy\n",
    "tot, correct, mseAcc, cosAcc, totFreq, freq = 0,0,0,0,0,0\n",
    "mse = torch.nn.MSELoss()\n",
    "cos = torch.nn.CosineEmbeddingLoss()\n",
    "for i,(x,y) in enumerate(test):\n",
    "    if i==10000:\n",
    "        break\n",
    "    x = Variable(x.float())\n",
    "    y = Variable(y.float())\n",
    "    outputs = net(x)\n",
    "    cosAcc += 1-cos(outputs, y,torch.tensor([[1.]])).detach().numpy()\n",
    "    mseAcc += mse(outputs, y).detach().numpy()\n",
    "    predicted, target = s2v.most_similar(y.numpy()[0], 1)[0][0], s2v.most_similar(outputs.detach().numpy()[0], 1)[0][0]\n",
    "    #s2v.most_similar(positive = [y.numpy()[0]])[0][0], s2v.most_similar(positive = [outputs.detach().numpy()[0]])[0][0]\n",
    "    correct += 1 if predicted.lower()==target.lower() or predicted.lower().replace(\"_\",\"\")==target.lower().replace(\"_\",\"\") else 0\n",
    "    tot+=1\n",
    "    totFreq+= s2v[target][0]\n",
    "    freq += s2v[target][0] if predicted.lower()==target.lower() or predicted.lower().replace(\"_\",\"\")==target.lower().replace(\"_\",\"\") else 0\n",
    "print(correct/tot)\n",
    "print(cosAcc/tot)\n",
    "print(mseAcc/tot)\n",
    "print(freq/totFreq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#forward cos verbs full nn\n",
    "print(correct/9999)\n",
    "print(cosAcc/9999)\n",
    "print(mseAcc/9999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2v.most_similar(s2v[\"man|NOUN\"][1], 1)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(totFreq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
